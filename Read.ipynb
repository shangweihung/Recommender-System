{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Acc(pred, y):\n",
    "    '''\n",
    "    Calculate accuracy\n",
    "    \n",
    "    Arges: pred(list): model prediction\n",
    "           y(list): ground truth\n",
    "    \n",
    "    Returns: Acc(float): Accuracy\n",
    "    '''\n",
    "    TP_ = np.logical_and(pred, y)\n",
    "    FP_ = np.logical_and(pred, np.logical_not(y))\n",
    "    TN_ = np.logical_and(np.logical_not(pred), np.logical_not(y))\n",
    "    FN_ = np.logical_and(np.logical_not(pred), y)\n",
    "\n",
    "    TP = sum(TP_)\n",
    "    FP = sum(FP_)\n",
    "    TN = sum(TN_)\n",
    "    FN = sum(FN_)\n",
    "\n",
    "    # accuracy\n",
    "    Acc = (TP + TN) / (TP + FP + TN + FN)\n",
    "    \n",
    "    return Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Processor():\n",
    "    def __init__(self, filename, mode ='val'):\n",
    "        '''\n",
    "        Initialization\n",
    "        '''\n",
    "        self.data = pd.read_csv(filename)\n",
    "        self.mode = mode\n",
    "        self.usersPerBook = defaultdict(set)\n",
    "        self.booksPerUser = defaultdict(set)\n",
    "        self.user_Map = defaultdict(int)\n",
    "        self.book_Map = defaultdict(int)\n",
    "        self.bookCount = defaultdict(int)\n",
    "        self.read = set()\n",
    "        self.totalRead = 0\n",
    "        \n",
    "        if mode ==  'val':\n",
    "            self.train_set = self.data[:190000]\n",
    "            self.val_set = self.data[190000:]\n",
    "            self.usersPerBook_train = defaultdict(set)\n",
    "            self.booksPerUser_train = defaultdict(set)\n",
    "            \n",
    "        elif mode == 'test':\n",
    "            self.train_set = self.data\n",
    "            \n",
    "    def preprocessing(self):\n",
    "        '''\n",
    "        Creater Dictionarys and Sets for the following tasks\n",
    "        '''\n",
    "        u_Count = 0\n",
    "        b_Count = 0\n",
    "        count = 0\n",
    "        for i in self.data.values:\n",
    "            user, book, r = i\n",
    "            if self.mode =='val' and count<190000:\n",
    "                self.usersPerBook_train[book].add(user)\n",
    "                self.booksPerUser_train[user].add(book)\n",
    "                \n",
    "            if user not in self.user_Map:\n",
    "                self.user_Map[user] = u_Count\n",
    "                u_Count += 1\n",
    "            if book not in self.book_Map:\n",
    "                self.book_Map[book] = b_Count\n",
    "                b_Count += 1\n",
    "            self.usersPerBook[book].add(user)\n",
    "            self.booksPerUser[user].add(book)\n",
    "            self.read.add((user,book))\n",
    "            count+=1\n",
    "            \n",
    "    def add_negative_samples(self, num, save_filename):\n",
    "        '''\n",
    "        Add negative samples into validation set\n",
    "        \n",
    "        Args: num(int): how many negative samples should be created\n",
    "              save_filename(str): the name of save file of validation data\n",
    "        '''\n",
    "        non_read= list()\n",
    "        count = 0\n",
    "\n",
    "        userID = list(self.booksPerUser_train.keys())\n",
    "        bookID = list(self.usersPerBook_train.keys())\n",
    "\n",
    "        while count < num:\n",
    "            choose_user = random.choice(list(userID))\n",
    "            choose_book = random.choice(list(bookID))\n",
    "            pair = (choose_user,choose_book)\n",
    "            if choose_book not in self.booksPerUser[choose_user]:\n",
    "                if pair not in non_read:\n",
    "                    non_read+=[(choose_user,choose_book)]\n",
    "                    count+=1\n",
    "                    \n",
    "        self.save_filename = save_filename        \n",
    "        with open(self.save_filename,'w+') as f:\n",
    "            for pos_d in self.val_set.values:\n",
    "                f.writelines(pos_d[0]+'-'+pos_d[1]+','+'1\\n')\n",
    "            for neg_d in non_read:\n",
    "                f.writelines(neg_d[0]+'-'+neg_d[1]+','+'0\\n')\n",
    "    \n",
    "    \n",
    "    def build_cosine_table(self):\n",
    "        '''\n",
    "        Build User-Book interaction matrix\n",
    "        '''\n",
    "        table = pd.pivot_table(self.train_set, values='rating', index=['userID'], columns=['bookID'], aggfunc=int)\n",
    "        table.fillna(0, inplace= True)\n",
    "        table.replace([1,2,3,4,5],[-1,-1,0,1,1], inplace= True)\n",
    "        self.users = list(table.index)\n",
    "        self.books = list(table.columns)\n",
    "\n",
    "        cosine_sim_user = cosine_similarity(table)\n",
    "        cosine_sim_book = cosine_similarity(table.T)\n",
    "\n",
    "        self.c_sim_book = pd.DataFrame(cosine_sim_book, index = self.books, columns = self.books )\n",
    "        self.c_sim_user = pd.DataFrame(cosine_sim_user, index = self.users, columns = self.users)\n",
    "\n",
    "  \n",
    "    def load_gamma(self, user_file_name, book_file_name):\n",
    "        '''\n",
    "        Load (bset) Gamma-matrix using one class recommendation algorithm\n",
    "        '''\n",
    "        self.gamma_u = np.load(user_file_name)\n",
    "        self.gamma_b = np.load(book_file_name)\n",
    "        \n",
    "    \n",
    "    def compute_gamma(self, n_epoch=40, latent_factor = 3, threshold = 0.0003, lr = 0.08):\n",
    "        '''\n",
    "        Compute gamma matrix using one class recommendation (latent factor)\n",
    "        '''\n",
    "    \n",
    "        self.gamma_u = np.random.normal(scale = 1./latent_factor, size = (len(self.booksPerUser), latent_factor))\n",
    "        self.gamma_b = np.random.normal(scale = 1./latent_factor, size = (len(self.usersPerBook), latent_factor))\n",
    "        \n",
    "        bookID = list(self.usersPerBook.keys())\n",
    "        dd = self.train_set.values.copy()\n",
    "        np.random.shuffle(dd)\n",
    "        old_acc, acc = 0, 0\n",
    "\n",
    "        for n in range(n_epoch):\n",
    "            old_acc = acc\n",
    "            for i in dd:   \n",
    "                user, book, r = i\n",
    "\n",
    "                nolook = random.choice(list(bookID))\n",
    "                while nolook in self.booksPerUser[user]:\n",
    "                    nolook = random.choice(list(bookID))\n",
    "\n",
    "\n",
    "                same_factor = np.exp(self.gamma_u[self.user_Map[user],:].dot(self.gamma_b[self.book_Map[nolook],:])\\\n",
    "                                     - self.gamma_u[self.user_Map[user],:].dot(self.gamma_b[self.book_Map[book],:]))\n",
    "                denom = 1 + same_factor\n",
    "\n",
    "                tmp =  self.gamma_u[self.user_Map[user],:]\n",
    "                self.gamma_u[self.user_Map[user],:] -= lr * (self.gamma_b[self.book_Map[nolook],:] - self.gamma_b[self.book_Map[book],:]) * same_factor / denom\n",
    "                self.gamma_b[self.book_Map[book],:] -= lr * (- tmp) * same_factor / denom\n",
    "                self.gamma_b[self.book_Map[nolook],:] -= lr * ( tmp) * same_factor / denom\n",
    "\n",
    "\n",
    "            with open(self.save_filename, 'r') as f:\n",
    "                count_acc = 0\n",
    "                for l in f:\n",
    "                    if l.startswith(\"userID\"):\n",
    "                        #header\n",
    "                        continue\n",
    "                    else:\n",
    "                        userbook, label = l.strip().split(',')\n",
    "                        u, b = userbook.split('-')\n",
    "\n",
    "                        pred = sigmoid(self.gamma_u[self.user_Map[u], :].dot(self.gamma_b[self.book_Map[b], :]))\n",
    "                        prediction = 1 if pred>=0.5 else 0\n",
    "                        count_acc += int(prediction == int(label))\n",
    "                acc = count_acc/ 20000.0\n",
    "                print('Epoch: %d - Acc: %f' % (n, acc))\n",
    "\n",
    "            if (old_acc - acc) >= threshold: break\n",
    "            np.random.shuffle(dd)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"assignment1/train_Interactions.csv.gz\"\n",
    "Read_Predictor = Data_Processor(filename)\n",
    "Read_Predictor.preprocessing()\n",
    "Read_Predictor.add_negative_samples(10000, 'assignment1/pairs_readbook_valid.txt')\n",
    "Read_Predictor.build_cosine_table()\n",
    "#Read_Predictor.load_gamma('gamma_u.npy','gamma_b.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Acc: 0.499900\n",
      "Epoch: 1 - Acc: 0.500150\n",
      "Epoch: 2 - Acc: 0.504000\n",
      "Epoch: 3 - Acc: 0.509950\n",
      "Epoch: 4 - Acc: 0.531900\n",
      "Epoch: 5 - Acc: 0.565550\n",
      "Epoch: 6 - Acc: 0.600950\n",
      "Epoch: 7 - Acc: 0.622250\n",
      "Epoch: 8 - Acc: 0.639850\n",
      "Epoch: 9 - Acc: 0.647600\n",
      "Epoch: 10 - Acc: 0.654100\n",
      "Epoch: 11 - Acc: 0.659200\n",
      "Epoch: 12 - Acc: 0.663750\n",
      "Epoch: 13 - Acc: 0.664450\n",
      "Epoch: 14 - Acc: 0.667800\n",
      "Epoch: 15 - Acc: 0.669900\n",
      "Epoch: 16 - Acc: 0.673700\n",
      "Epoch: 17 - Acc: 0.672950\n"
     ]
    }
   ],
   "source": [
    "Read_Predictor.compute_gamma()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "def cosine_similarity_book(b, b_other):\n",
    "    '''\n",
    "    Return item-based cosine similarity\n",
    "    \n",
    "    Args: b(str): validated book\n",
    "          b_other(str): other books\n",
    "    '''\n",
    "    \n",
    "    if b in Read_Predictor.books:\n",
    "        return Read_Predictor.c_sim_book.loc[b][b_other]\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def cosine_similarity_user(u, u_other):\n",
    "    '''\n",
    "    Return user-based cosine similarity\n",
    "    \n",
    "    Args: u(str): validated user\n",
    "          u_other(str): other users\n",
    "    '''\n",
    "    \n",
    "    if u in Read_Predictor.users:\n",
    "        return Read_Predictor.c_sim_user.loc[u][u_other]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# item_based\n",
    "def cosine_mostSimilar_book(u, b, usersPerBook_, booksPerUser_, decide_on = 1):\n",
    "    '''\n",
    "    Giver a pair(user, book), compute the Cosine Similarity betwwen b and b'\n",
    "    \n",
    "    Arges: u(string): user\n",
    "           b(string): book\n",
    "           decide_on(int): the number of maximum similarities to be considered\n",
    "                           (default 1)\n",
    "           usersPerBook_(dict): set of users for each book\n",
    "           booksPerUser_(dict): set of books for each user\n",
    "    Returns:\n",
    "           Avg of biggest similarities with respect to the number of decide_on\n",
    "    \n",
    "    '''\n",
    "    similarities = []\n",
    "    \n",
    "    candidateItems = set()\n",
    "    \n",
    "    for book in booksPerUser_[u]:\n",
    "        if book == b: continue\n",
    "        candidateItems.add(book)\n",
    "\n",
    "    ## candidateItems: b'\n",
    "    for i2 in candidateItems:\n",
    "        sim = cosine_similarity_book(b, i2)\n",
    "        similarities.append(sim)\n",
    "    \n",
    " \n",
    "    similarities.sort(reverse=True)\n",
    "    if decide_on == 1:\n",
    "        return max(similarities) if similarities else 0 \n",
    "    else:        \n",
    "        if len(similarities)<=decide_on:\n",
    "            return sum(similarities)/len(similarities)  if similarities else 0\n",
    "        else:\n",
    "            return sum(similarities[:decide_on])/decide_on\n",
    "        \n",
    "# user_based\n",
    "def cosine_mostSimilar_user(u, b, booksPerUser_, usersPerBook_, decide_on=1):\n",
    "    '''\n",
    "    Giver a pair(user, book), compute the Cosine Similarity betwwen u and u'\n",
    "    \n",
    "    Arges: u(string): user\n",
    "           b(string): book\n",
    "           decide_on(int): the number of maximum similarities to be considered\n",
    "                           (default 1)\n",
    "           booksPerUser_(dict): set of books for each user\n",
    "           usersPerBook_(dict): set of users for each book\n",
    "    Returns:\n",
    "           Avg of biggest similarities with respect to the number of decide_on\n",
    "    \n",
    "    '''\n",
    "    similarities = []\n",
    "\n",
    "    candidateUsers = set()\n",
    "    \n",
    "    for user in usersPerBook_[b]:\n",
    "        if user == u: continue\n",
    "        candidateUsers.add(user)\n",
    "        \n",
    "    ## candidateUsers: u'\n",
    "    for u2 in candidateUsers:\n",
    "        sim = cosine_similarity_user(u,u2)\n",
    "        similarities.append(sim)\n",
    "        \n",
    "    similarities.sort(reverse=True)\n",
    "    if decide_on == 1:\n",
    "        return max(similarities) if similarities else 0 \n",
    "    else:        \n",
    "        if len(similarities)<=decide_on:\n",
    "            return sum(similarities)/len(similarities)  if similarities else 0\n",
    "        else:\n",
    "            return sum(similarities[:decide_on])/decide_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    '''\n",
    "    Calculate Jaccard similarity\n",
    "    Args: s1, s2 (set): two sets to be calculated\n",
    "    \n",
    "    Returns: similarity (float)\n",
    "    \n",
    "    '''\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom\n",
    "\n",
    "# item_based\n",
    "def jaccard_mostSimilar_book(u, b, usersPerBook_, booksPerUser_, decide_on = 1):\n",
    "    '''\n",
    "    Giver a pair(user, book), compute the Jaccard Similarity betwwen b and b'\n",
    "    \n",
    "    Arges: u(string): user\n",
    "           b(string): book\n",
    "           decide_on(int): the number of maximum similarities to be considered\n",
    "                           (default 1)\n",
    "           usersPerBook_(dict): set of users for each book\n",
    "           booksPerUser_(dict): set of books for each user\n",
    "    Returns:\n",
    "           Avg of biggest similarities with respect to the number of decide_on\n",
    "    \n",
    "    '''\n",
    "    similarities = []\n",
    "    users = usersPerBook_[b]\n",
    "    \n",
    "    candidateItems = set()\n",
    "    \n",
    "    for book in booksPerUser_[u]:\n",
    "        if book == b: continue\n",
    "        candidateItems.add(book)\n",
    "\n",
    "    ## candidateItems: b'\n",
    "    for i2 in candidateItems:\n",
    "        if i2 == b: continue\n",
    "        sim = Jaccard(users, usersPerBook_[i2])\n",
    "        similarities.append(sim)\n",
    "    \n",
    "    similarities.sort(reverse=True)\n",
    "    if decide_on == 1:\n",
    "        return max(similarities) if similarities else 0 \n",
    "    else:        \n",
    "        if len(similarities)<=decide_on:\n",
    "            return sum(similarities)/len(similarities)  if similarities else 0\n",
    "        else:\n",
    "            return sum(similarities[:decide_on])/decide_on\n",
    "\n",
    "# user_based\n",
    "def jaccard_mostSimilar_user(u, b, booksPerUser_, usersPerBook_, decide_on=1):\n",
    "    '''\n",
    "    Giver a pair(user, book), compute the Jaccard Similarity betwwen u and u'\n",
    "    \n",
    "    Arges: u(string): user\n",
    "           b(string): book\n",
    "           decide_on(int): the number of maximum similarities to be considered\n",
    "                           (default 1)\n",
    "           booksPerUser_(dict): set of books for each user\n",
    "           usersPerBook_(dict): set of users for each book\n",
    "    Returns:\n",
    "           Avg of biggest similarities with respect to the number of decide_on\n",
    "    \n",
    "    '''\n",
    "    similarities = []\n",
    "    books = booksPerUser_[u]\n",
    "\n",
    "    candidateUsers = set()\n",
    "    for user in usersPerBook_[b]:\n",
    "        if user == u: continue\n",
    "        candidateUsers.add(user)\n",
    "        \n",
    "    ## candidateUsers: u'\n",
    "    for u2 in candidateUsers:\n",
    "        if u2 == u: continue\n",
    "        sim = Jaccard(books,booksPerUser_[u2])\n",
    "        similarities.append(sim)\n",
    "        \n",
    "    similarities.sort(reverse=True)\n",
    "    if decide_on == 1:\n",
    "        return max(similarities) if similarities else 0 \n",
    "    else:        \n",
    "        if len(similarities)<=decide_on:\n",
    "            return sum(similarities)/len(similarities)  if similarities else 0\n",
    "        else:\n",
    "            return sum(similarities[:decide_on])/decide_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating each features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"assignment1/pairs_readbook_valid.txt\", 'r') as f:\n",
    "    dissimilar = []\n",
    "    popularity = []\n",
    "    c_s_book = []\n",
    "    c_s_user = []\n",
    "    jar_s_book = []\n",
    "    jar_s_user = []\n",
    "    label = []\n",
    "    for l in f:\n",
    "        if l.startswith(\"userID\"):\n",
    "            #header\n",
    "            continue\n",
    "        else:\n",
    "            userbook, lab = l.strip().split(',')\n",
    "            user, book = userbook.split('-')\n",
    "\n",
    "            pred = sigmoid(Read_Predictor.gamma_u[Read_Predictor.user_Map[user], :].dot(Read_Predictor.gamma_b[Read_Predictor.book_Map[book], :]))\n",
    "            dissimilar.append(1 if pred>=0.5 else 0)\n",
    "        \n",
    "            # popularity.append(int(book in return_best))  \n",
    "            count = Read_Predictor.bookCount[book] if Read_Predictor.bookCount[book] else 0\n",
    "            popularity.append(count)\n",
    "            \n",
    "            jar_s_book.append(jaccard_mostSimilar_book(user, book, Read_Predictor.usersPerBook_train, Read_Predictor.booksPerUser_train, 15))\n",
    "            jar_s_user.append(jaccard_mostSimilar_user(user, book, Read_Predictor.booksPerUser_train, Read_Predictor.usersPerBook_train, 15))\n",
    "            c_s_book.append(cosine_mostSimilar_book(user, book, Read_Predictor.usersPerBook_train, Read_Predictor.booksPerUser_train, 15))\n",
    "            c_s_user.append(cosine_mostSimilar_user(user, book, Read_Predictor.booksPerUser_train, Read_Predictor.usersPerBook_train, 15))\n",
    "            \n",
    "            label.append(int(lab))\n",
    "\n",
    "popularity = [np.log(p) if p != 0 else 0 for p in popularity]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack((popularity, jar_s_book, jar_s_user, c_s_book, c_s_user, dissimilar))\n",
    "Y = np.array((label))\n",
    "\n",
    "# Shuffle the data\n",
    "XY = list(zip(X,Y))\n",
    "random.shuffle(XY)\n",
    "\n",
    "X = [s[0] for s in XY]\n",
    "Y = [s[1] for s in XY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-1: 0.7099\n",
      "1-2: 0.7094500000000001\n",
      "1-3: 0.7093999999999999\n"
     ]
    }
   ],
   "source": [
    "clf1_1 = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
    "clf1_1.fit(X,Y)\n",
    "clf1_1_score = cross_val_score(clf1_1, X, Y, cv=5) \n",
    "\n",
    "clf1_2 = RandomForestClassifier(n_estimators=150, max_depth=5, random_state=0)\n",
    "clf1_2.fit(X,Y)\n",
    "clf1_2_score = cross_val_score(clf1_2, X, Y, cv=5)\n",
    "\n",
    "clf1_3 = RandomForestClassifier(n_estimators=200, max_depth=5, random_state=0)\n",
    "clf1_3.fit(X,Y) \n",
    "clf1_3_score = cross_val_score(clf1_3, X, Y, cv=5)\n",
    "\n",
    "print('1-1:',clf1_1_score.mean())\n",
    "print('1-2:',clf1_2_score.mean())\n",
    "print('1-3:',clf1_3_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-1: 0.67365\n",
      "2-2: 0.7031499999999999\n",
      "2-3: 0.7046499999999999\n"
     ]
    }
   ],
   "source": [
    "clf2_1 = LogisticRegression(random_state=0, solver='lbfgs',class_weight ='balanced')\n",
    "clf2_1.fit(X, Y)\n",
    "clf2_1_score = cross_val_score(clf2_1, X, Y, cv=5)\n",
    "\n",
    "clf2_2 = LogisticRegression(random_state=0, solver='lbfgs',class_weight ='balanced', C=50)\n",
    "clf2_2.fit(X, Y)\n",
    "clf2_2_score = cross_val_score(clf2_2, X, Y, cv=5)\n",
    "\n",
    "clf2_3 = LogisticRegression(random_state=0, solver='lbfgs',class_weight ='balanced', C=100)\n",
    "clf2_3.fit(X, Y)\n",
    "clf2_3_score = cross_val_score(clf2_3, X, Y, cv=5)\n",
    " \n",
    "print('2-1:',clf2_1_score.mean())\n",
    "print('2-2:',clf2_2_score.mean())\n",
    "print('2-3:',clf2_3_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-1: 0.7077000000000001\n",
      "3-2: 0.70735\n",
      "3-3: 0.7072499999999999\n"
     ]
    }
   ],
   "source": [
    "clf3_1 = AdaBoostClassifier(n_estimators=100)\n",
    "clf3_1.fit(X, Y)\n",
    "clf3_1_score = cross_val_score(clf3_1, X, Y, cv=5) \n",
    "\n",
    "clf3_2 = AdaBoostClassifier(n_estimators=50)\n",
    "clf3_2.fit(X,Y)\n",
    "clf3_2_score = cross_val_score(clf3_2, X, Y, cv=5) \n",
    "\n",
    "clf3_3 = AdaBoostClassifier(n_estimators=25)\n",
    "clf3_3.fit(X,Y)\n",
    "clf3_3_score = cross_val_score(clf3_3, X, Y, cv=5) \n",
    "\n",
    "print('3-1:',clf3_1_score.mean())\n",
    "print('3-2:',clf3_2_score.mean())\n",
    "print('3-3:',clf3_3_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-1: 0.70795\n",
      "4-2: 0.70795\n",
      "4-3: 0.70795\n"
     ]
    }
   ],
   "source": [
    "clf4_1 = GradientBoostingClassifier(n_estimators=100)\n",
    "clf4_1.fit(X, Y)\n",
    "clf4_1_score = cross_val_score(clf4_1, X, Y, cv=5) \n",
    "\n",
    "clf4_2 = GradientBoostingClassifier(n_estimators=80)\n",
    "clf4_2.fit(X, Y)\n",
    "clf4_2_score = cross_val_score(clf4_2, X, Y, cv=5) \n",
    "\n",
    "clf4_3 = GradientBoostingClassifier(n_estimators=60)\n",
    "clf4_3.fit(X, Y)\n",
    "clf4_3_score = cross_val_score(clf4_3, X, Y, cv=5) \n",
    "\n",
    "print('4-1:',clf4_1_score.mean())\n",
    "print('4-2:',clf4_1_score.mean())\n",
    "print('4-3:',clf4_1_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"assignment1/train_Interactions.csv.gz\"\n",
    "Read_Predictor = Data_Processor(filename, mode='test')\n",
    "Read_Predictor.preprocessing()\n",
    "Read_Predictor.build_cosine_table()\n",
    "Read_Predictor.load_gamma('gamma_u.npy','gamma_b.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"assignment1/pairs_Read.txt\") as pair_read:\n",
    "    dissimilar_test = []\n",
    "    popularity_test = []\n",
    "    jar_s_book_test = []\n",
    "    jar_s_user_test = []\n",
    "    c_s_book_test = []\n",
    "    c_s_user_test = []\n",
    "    \n",
    "    for l in pair_read:\n",
    "        if l.startswith(\"userID\"):\n",
    "            continue\n",
    "        else:\n",
    "            user, book = l.strip().split('-')\n",
    "            try:\n",
    "                pred = sigmoid(Read_Predictor.gamma_u[Read_Predictor.user_Map[user], :]\\\n",
    "                               .dot(Read_Predictor.gamma_b[Read_Predictor.book_Map[book], :]))\n",
    "                dissimilar_test.append(1 if pred>=0.5 else 0)\n",
    "            except:\n",
    "                dissimilar_test.append(0)\n",
    "            \n",
    "            count = Read_Predictor.bookCount[book] if Read_Predictor.bookCount[book] else 0\n",
    "            popularity_test.append(count)\n",
    "            jar_s_book_test.append(jaccard_mostSimilar_book(user, book, Read_Predictor.usersPerBook, Read_Predictor.booksPerUser, 15))\n",
    "            jar_s_user_test.append(jaccard_mostSimilar_user(user, book, Read_Predictor.booksPerUser, Read_Predictor.usersPerBook, 15))\n",
    "            c_s_book_test.append(cosine_mostSimilar_book(user, book, Read_Predictor.usersPerBook, Read_Predictor.booksPerUser, 15))\n",
    "            c_s_user_test.append(cosine_mostSimilar_user(user, book, Read_Predictor.booksPerUser, Read_Predictor.usersPerBook, 15))\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity_test = [np.log(p) if p != 0 else 0 for p in popularity_test]\n",
    "X_test = np.column_stack((popularity_test, jar_s_book_test, jar_s_user_test, c_s_book_test, c_s_user_test, dissimilar_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = []\n",
    "with open(\"assignment1/predictions_Read.txt\", 'w') as predictions:\n",
    "    idx = 0\n",
    "    for l in open(\"assignment1/pairs_Read.txt\"):\n",
    "        if l.startswith(\"userID\"):\n",
    "            #header\n",
    "            predictions.write(l)\n",
    "            continue\n",
    "        else:\n",
    "            user, book = l.strip().split('-')\n",
    "            test_features = X_test[idx,:].reshape(-1,6)\n",
    "            \n",
    "            if clf4_3.predict(test_features)[0] == 1:\n",
    "                check.append(1)\n",
    "                predictions.write(user + '-' + book + \",1\\n\")\n",
    "            else:\n",
    "                check.append(0)\n",
    "                predictions.write(user + '-' + book + \",0\\n\")\n",
    "            \n",
    "            idx +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 9211, 1: 10789})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(check)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
